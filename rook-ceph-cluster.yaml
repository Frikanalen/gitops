apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: rook-ceph-cluster
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://charts.rook.io/release
    chart: rook-ceph-cluster
    targetRevision: v1.17
    helm:
      values: |
        configOverride: |
          [global]
          osd_pool_default_size = 1
          osd_pool_default_min_size = 1

        toolbox:
          enabled: true
        cephClusterSpec:
          storage:
            useAllDevices: false
            useAllNodes: false
            nodes:
              - name: tx2
                devices:
                  - name: "/dev/disk/by-partuuid/45b774bf-df61-4af1-b702-a4564e9091e0"
          mon:
            allowMultiplePerNode: true
          mgr:
            allowMultiplePerNode: true
          dashboard:
            enabled: true
          monitoring:
            enabled: true

        cephBlockPools:
          - name: ceph-blockpool
            # see https://github.com/rook/rook/blob/master/Documentation/CRDs/Block-Storage/ceph-block-pool-crd.md#spec for available configuration
            spec:
              failureDomain: host
              replicated:
                size: 1
                requireSafeReplicaSize: false
              # Enables collecting RBD per-image IO statistics by enabling dynamic OSD performance counters. Defaults to false.
              # For reference: https://docs.ceph.com/docs/latest/mgr/prometheus/#rbd-io-statistics
              # enableRBDStats: true
            storageClass:
              enabled: true
              name: ceph-block
              annotations: {}
              labels: {}
              isDefault: true
              reclaimPolicy: Delete
              allowVolumeExpansion: true
              volumeBindingMode: "Immediate"
              mountOptions: []
              # see https://kubernetes.io/docs/concepts/storage/storage-classes/#allowed-topologies
              allowedTopologies: []
              #        - matchLabelExpressions:
              #            - key: rook-ceph-role
              #              values:
              #                - storage-node
              # see https://github.com/rook/rook/blob/master/Documentation/Storage-Configuration/Block-Storage-RBD/block-storage.md#provision-storage for available configuration
              parameters:
                # (optional) mapOptions is a comma-separated list of map options.
                # For krbd options refer
                # https://docs.ceph.com/docs/latest/man/8/rbd/#kernel-rbd-krbd-options
                # For nbd options refer
                # https://docs.ceph.com/docs/latest/man/8/rbd-nbd/#options
                # mapOptions: lock_on_read,queue_depth=1024

                # (optional) unmapOptions is a comma-separated list of unmap options.
                # For krbd options refer
                # https://docs.ceph.com/docs/latest/man/8/rbd/#kernel-rbd-krbd-options
                # For nbd options refer
                # https://docs.ceph.com/docs/latest/man/8/rbd-nbd/#options
                # unmapOptions: force

                # RBD image format. Defaults to "2".
                imageFormat: "2"

                # RBD image features, equivalent to OR'd bitfield value: 63
                # Available for imageFormat: "2". Older releases of CSI RBD
                # support only the `layering` feature. The Linux kernel (KRBD) supports the
                # full feature complement as of 5.4
                imageFeatures: layering

                # These secrets contain Ceph admin credentials.
                csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
                csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
                csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
                csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
                csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
                csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
                # Specify the filesystem type of the volume. If not specified, csi-provisioner
                # will set default as `ext4`. Note that `xfs` is not recommended due to potential deadlock
                # in hyperconverged settings where the volume is mounted on the same node as the osds.
                csi.storage.k8s.io/fstype: ext4

        # -- A list of CephFileSystem configurations to deploy
        # @default -- See [below](#ceph-file-systems)
        cephFileSystems:
          - name: ceph-filesystem
            # see https://github.com/rook/rook/blob/master/Documentation/CRDs/Shared-Filesystem/ceph-filesystem-crd.md#filesystem-settings for available configuration
            spec:
              metadataPool:
                replicated:
                  size: 1
                  requireSafeReplicaSize: false
              dataPools:
                - failureDomain: host
                  replicated:
                    size: 1
                    requireSafeReplicaSize: false
                  name: data0
              metadataServer:
                activeCount: 1
                activeStandby: true
                resources:
                  limits:
                    memory: "4Gi"
                  requests:
                    cpu: "1000m"
                    memory: "4Gi"
                priorityClassName: system-cluster-critical
            storageClass:
              enabled: true
              isDefault: false
              name: ceph-filesystem
              # (Optional) specify a data pool to use, must be the name of one of the data pools above, 'data0' by default
              pool: data0
              reclaimPolicy: Delete
              allowVolumeExpansion: true
              volumeBindingMode: "Immediate"
              annotations: {}
              labels: {}
              mountOptions: []
              # see https://github.com/rook/rook/blob/master/Documentation/Storage-Configuration/Shared-Filesystem-CephFS/filesystem-storage.md#provision-storage for available configuration
              parameters:
                # The secrets contain Ceph admin credentials.
                csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
                csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
                csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
                csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
                csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
                csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
                # Specify the filesystem type of the volume. If not specified, csi-provisioner
                # will set default as `ext4`. Note that `xfs` is not recommended due to potential deadlock
                # in hyperconverged settings where the volume is mounted on the same node as the osds.
                csi.storage.k8s.io/fstype: ext4


        # -- A list of CephObjectStore configurations to deploy
        # @default -- See [below](#ceph-object-stores)
        cephObjectStores:
          - name: ceph-objectstore
            # see https://github.com/rook/rook/blob/master/Documentation/CRDs/Object-Storage/ceph-object-store-crd.md#object-store-settings for available configuration
            spec:
              metadataPool:
                failureDomain: host
                replicated:
                  size: 1
                  requireSafeReplicaSize: false
              dataPool:
                replicated:
                  size: 1
                  requireSafeReplicaSize: false
                failureDomain: host
                parameters:
                  bulk: "true"
              preservePoolsOnDelete: true
              gateway:
                port: 80
                resources:
                  limits:
                    memory: "2Gi"
                  requests:
                    cpu: "1000m"
                    memory: "1Gi"
                # securePort: 443
                # sslCertificateRef:
                instances: 1
                priorityClassName: system-cluster-critical
                # opsLogSidecar:
                #   resources:
                #     limits:
                #       memory: "100Mi"
                #     requests:
                #       cpu: "100m"
                #       memory: "40Mi"
            storageClass:
              enabled: true
              name: ceph-bucket
              reclaimPolicy: Delete
              volumeBindingMode: "Immediate"
              annotations: {}
              labels: {}
              # see https://github.com/rook/rook/blob/master/Documentation/Storage-Configuration/Object-Storage-RGW/ceph-object-bucket-claim.md#storageclass for available configuration
              parameters:
                # note: objectStoreNamespace and objectStoreName are configured by the chart
                region: us-east-1
            ingress:
              # Enable an ingress for the ceph-objectstore
              enabled: false
              # The ingress port by default will be the object store's "securePort" (if set), or the gateway "port".
              # To override those defaults, set this ingress port to the desired port.
              # port: 80
              # annotations: {}
              # host:
              #   name: objectstore.example.com
              #   path: /
              #   pathType: Prefix
              # tls:
              # - hosts:
              #     - objectstore.example.com
              #   secretName: ceph-objectstore-tls
              # ingressClassName: nginx
      parameters:
        - name: operatorNamespace
          value: rook-ceph
  destination:
    server: https://kubernetes.default.svc
    namespace: rook-ceph
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true

